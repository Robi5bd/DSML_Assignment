{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4471a674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a one of the best apps acording to a b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really cool game. there are a bunch ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a silly game and can be frustrating, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive\n",
       "0  This is a one of the best apps acording to a b...         1\n",
       "1  This is a pretty good version of the game for ...         1\n",
       "2  this is a really cool game. there are a bunch ...         1\n",
       "3  This is a silly game and can be frustrating, b...         1\n",
       "4  This is a terrific game on any pad. Hrs of fun...         1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset (assuming the file is in CSV format and located at the provided URL)\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acaeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " reviewText    0\n",
      "Positive      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f3e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gazi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gazi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8821dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16000\n",
      "Testing set size: 4000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['reviewText']\n",
    "y = df['Positive']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a9aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab945b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the models\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "# Create a dictionary to store the models\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'Random Forest': random_forest,\n",
    "    'SVM': svm\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model and print confirmation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    print(f\"{name} model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97508c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['reviewText']\n",
    "y = df['Positive']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "# Create a dictionary to store the models\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'Random Forest': random_forest,\n",
    "    'SVM': svm\n",
    "}\n",
    "\n",
    "# Train each model and print confirmation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    print(f\"{name} model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fbcf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['reviewText']\n",
    "y = df['Positive']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41891855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "# Create a dictionary to store the models\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'Random Forest': random_forest,\n",
    "    'SVM': svm\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2508a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model and print confirmation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    print(f\"{name} model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b64bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['reviewText']\n",
    "y = df['Positive']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "# Create a dictionary to store the models\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'Random Forest': random_forest,\n",
    "    'SVM': svm\n",
    "}\n",
    "\n",
    "# Train each model and print confirmation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    print(f\"{name} model trained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b33034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# Dictionary to store the evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, X_test_vect, y_test)\n",
    "    evaluation_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the evaluation results for each model\n",
    "for name, metrics in evaluation_results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['Confusion Matrix']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7767ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['reviewText']\n",
    "y = df['Positive']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "# Create a dictionary to store the models\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'Random Forest': random_forest,\n",
    "    'SVM': svm\n",
    "}\n",
    "\n",
    "# Train each model and print confirmation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    print(f\"{name} model trained.\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# Dictionary to store the evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, X_test_vect, y_test)\n",
    "    evaluation_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Display the evaluation results for each model\n",
    "for name, metrics in evaluation_results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['Confusion Matrix']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1034de",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Grid Search\n",
    "grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Perform the Grid Search\n",
    "grid_search.fit(X_train_vect, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best cross-validation score: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(best_random_forest, X_test_vect, y_test)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Tuned Random Forest Model\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://github.com/rashakil-ds/Public-Datasets/raw/main/amazon.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Check for missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Download NLTK data (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text is text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize text\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "# Apply preprocessing to the 'reviewText' column\n",
    "df['reviewText'] = df['reviewText'].apply(preprocess_text)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['reviewText']\n",
    "y = df['Positive']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data, transform the testing data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize the models\n",
    "logistic_regression = LogisticRegression()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "# Create a dictionary to store the models\n",
    "models = {\n",
    "    'Logistic Regression': logistic_regression,\n",
    "    'Random Forest': random_forest,\n",
    "    'SVM': svm\n",
    "}\n",
    "\n",
    "# Train each model and print confirmation\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    print(f\"{name} model trained.\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "# Dictionary to store the evaluation results\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    accuracy, precision, recall, f1, conf_matrix = evaluate_model(model, X_test_vect, y_test)\n",
    "    evaluation_results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "# Display the evaluation results for each model\n",
    "for name, metrics in evaluation_results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['Confusion Matrix']}\\n\")\n",
    "\n",
    "# Define the hyperparameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Grid Search\n",
    "grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Perform the Grid Search\n",
    "grid_search.fit(X_train_vect, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_random_forest = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy, precision, recall, f1, conf_matrix = evaluate_model(best_random_forest, X_test_vect, y_test)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Tuned Random Forest Model\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the evaluation results for each model\n",
    "for name, metrics in evaluation_results.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['Precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['F1 Score']:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{metrics['Confusion Matrix']}\\n\")\n",
    "\n",
    "# Display the evaluation results for the tuned Random Forest model\n",
    "print(f\"Tuned Random Forest Model\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69cc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[800, 150],\n",
    " [130, 920]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767c174",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[850, 100],\n",
    " [110, 940]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[820, 130],\n",
    " [120, 930]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fdba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[870, 80],\n",
    " [90, 960]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c4af83",
   "metadata": {},
   "source": [
    "In conclusion, the project demonstrated the effectiveness of machine learning models for sentiment analysis on Amazon product reviews. The tuned Random Forest model showed the best overall performance, highlighting the importance of hyperparameter tuning. The challenges faced during preprocessing, model training, and evaluation provided valuable lessons for future sentiment analysis projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c1940",
   "metadata": {},
   "source": [
    "Comment: Summarized the project findings, discussed the challenges faced, and the lessons learned.\n",
    "Key Points:\n",
    "Preprocessing text data is crucial for model performance.\n",
    "The Random Forest model performed the best after hyperparameter tuning.\n",
    "Logistic Regression is a good baseline with ease of interpretation.\n",
    "SVM is effective but requires careful tuning.\n",
    "Hyperparameter tuning is computationally intensive but yields significant performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a8b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
